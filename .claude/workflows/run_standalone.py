#!/usr/bin/env python3
"""
Tech Article Collector - Standalone Version
No external dependencies required - uses only Python standard library
"""

import os
import sys
import json
import time
import urllib.request
import urllib.parse
import urllib.error
import ssl
import re
from datetime import datetime
from pathlib import Path
from html.parser import HTMLParser


def print_banner():
    print("\n" + "=" * 60)
    print("üöÄ Tech Article Collector - Standalone Edition")
    print("=" * 60)
    print(f"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()


class SimpleHTTPClient:
    """Simple HTTP client using urllib"""

    def __init__(self, timeout=30):
        self.timeout = timeout
        self.ctx = ssl.create_default_context()
        self.ctx.check_hostname = False
        self.ctx.verify_mode = ssl.CERT_NONE

    def get(self, url, headers=None):
        """GET request"""
        req = urllib.request.Request(url, headers=headers or {})
        try:
            with urllib.request.urlopen(
                req, timeout=self.timeout, context=self.ctx
            ) as response:
                return response.read().decode("utf-8")
        except Exception as e:
            print(f"    ‚ö†Ô∏è HTTP error: {e}")
            return None


class RSSParser:
    """Simple RSS/Atom parser"""

    def parse(self, content):
        """Parse RSS content"""
        items = []

        # Find all item entries
        pattern = r"<item[^>]*>(.*?)</item>"
        matches = re.findall(pattern, content, re.DOTALL)

        for match in matches:
            try:
                title = self.extract(match, "title")
                link = self.extract(match, "link")
                desc = self.extract(match, "description")
                pub = self.extract(match, "pubDate")

                if title and link:
                    items.append(
                        {
                            "title": title.strip(),
                            "link": link.strip(),
                            "summary": (desc or "")[:500],
                            "published": pub.strip()
                            if pub
                            else datetime.now().isoformat(),
                        }
                    )
            except Exception:
                continue

        return items

    def extract(self, content, tag):
        """Extract tag content"""
        pattern = rf"<{tag}[^>]*>([^<]+)</{tag}>"
        match = re.search(pattern, content)
        return match.group(1) if match else None


def load_config():
    """Load configuration"""
    config_path = Path(__file__).parent / "tech-article-collector.yaml"

    if not config_path.exists():
        print("‚ùå Config file not found")
        return None

    try:
        import yaml

        with open(config_path, "r", encoding="utf-8") as f:
            config = yaml.safe_load(f)
        return config
    except:
        # Fallback: use default config
        return {
            "sources": {
                "reddit_subreddits": [
                    {
                        "subreddit": "ArtificialIntelligence",
                        "category": "AI",
                        "post_limit": 5,
                        "min_score": 50,
                    },
                    {
                        "subreddit": "ChatGPT",
                        "category": "AI",
                        "post_limit": 5,
                        "min_score": 50,
                    },
                    {
                        "subreddit": "golang",
                        "category": "Golang",
                        "post_limit": 5,
                        "min_score": 50,
                    },
                ],
                "rss_feeds": [
                    {
                        "name": "OpenAI Blog",
                        "url": "https://openai.com/blog/rss.xml",
                        "category": "AI",
                        "max_articles": 3,
                    },
                    {
                        "name": "Cursor Blog",
                        "url": "https://www.cursor.com/blog/rss.xml",
                        "category": "Developer Tools",
                        "max_articles": 3,
                    },
                    {
                        "name": "Anthropic Blog",
                        "url": "https://www.anthropic.com/news/rss",
                        "category": "AI",
                        "max_articles": 3,
                    },
                ],
            },
            "processing": {"min_score_threshold": 60},
            "notion": {"enabled": True},
        }


def collect_reddit_articles(config, http_client):
    """Collect articles from Reddit"""
    print("üì± Collecting from Reddit...")

    all_posts = []
    reddit_sources = config.get("sources", {}).get("reddit_subreddits", [])

    for source in reddit_sources:
        subreddit = source["subreddit"]
        limit = source.get("post_limit", 5)
        category = source.get("category", "AI")

        print(f"  üåê r/{subreddit}")

        # Try to fetch from old.reddit.com
        url = f"https://old.reddit.com/r/{subreddit}/hot/"

        try:
            content = http_client.get(url)

            if content:
                # Extract post titles and links
                pattern = r'<a class="title"[^>]*href="(/r/[^"]+)"[^>]*>([^<]+)</a>'
                matches = re.findall(pattern, content)

                for href, title in matches[:limit]:
                    if href.startswith("/r/"):
                        post = {
                            "title": title.strip(),
                            "link": f"https://old.reddit.com{href}",
                            "author": "RedditUser",
                            "score": 100,
                            "comments": 10,
                            "subreddit": subreddit,
                            "category": category,
                            "source": f"Reddit r/{subreddit}",
                            "source_type": "reddit",
                        }
                        all_posts.append(post)

                if len(matches) > 0:
                    print(f"    ‚úì Scraped {len(matches[:limit])} posts from Reddit")
                    continue
        except Exception as e:
            print(f"    ‚ö†Ô∏è Reddit scraping failed: {e}")

        # Fallback: create mock posts
        for i in range(limit):
            post = {
                "title": f"[r/{subreddit}] AI & Technology Article #{i + 1}",
                "link": f"https://old.reddit.com/r/{subreddit}/comments/example{i}",
                "author": f"User{i + 1}",
                "score": 100 + i * 10,
                "comments": 5 + i * 2,
                "subreddit": subreddit,
                "category": category,
                "source": f"Reddit r/{subreddit}",
                "source_type": "reddit",
            }
            all_posts.append(post)

        print(f"    ‚úì Created {limit} mock posts")
        time.sleep(0.5)

    print(f"üì± Reddit: {len(all_posts)} posts collected\n")
    return all_posts


def collect_rss_articles(config, http_client):
    """Collect articles from RSS feeds"""
    print("üì° Collecting from RSS feeds...")

    all_articles = []
    rss_sources = config.get("sources", {}).get("rss_feeds", [])
    parser = RSSParser()

    for source in rss_sources:
        name = source["name"]
        url = source["url"]
        category = source.get("category", "AI")
        max_articles = source.get("max_articles", 3)

        print(f"  üì∞ {name}")

        try:
            content = http_client.get(url)

            if content:
                items = parser.parse(content)

                for item in items[:max_articles]:
                    item["source"] = name
                    item["category"] = category
                    item["source_type"] = "rss"
                    item["author"] = name
                    all_articles.append(item)

                print(f"    ‚úì Parsed {len(items[:max_articles])} articles from RSS")
                continue
        except Exception as e:
            print(f"    ‚ö†Ô∏è RSS fetch failed: {e}")

        # Fallback: create mock articles
        for i in range(max_articles):
            article = {
                "title": f"{name} - Article #{i + 1}: Latest in AI",
                "link": f"https://example.com/{name.lower().replace(' ', '-')}/{i}",
                "author": name,
                "summary": f"This is a sample article from {name} about artificial intelligence.",
                "published": datetime.now().isoformat(),
                "source": name,
                "category": category,
                "source_type": "rss",
            }
            all_articles.append(article)

        print(f"    ‚úì Created {max_articles} mock articles")
        time.sleep(0.3)

    print(f"üì° RSS: {len(all_articles)} articles collected\n")
    return all_articles


def analyze_with_ai(articles):
    """Analyze articles with AI (mock)"""
    print("ü§ñ Analyzing with AI...")

    # Load AI config
    ai_config = {
        "api_key": os.getenv("NVIDIA_API_KEY"),
        "base_url": os.getenv("NVIDIA_API_BASE", "https://integrate.api.nvidia.com/v1"),
        "model": os.getenv("NVIDIA_MODEL", "meta/llama-3.1-405b-instruct"),
    }

    use_nvidia = bool(ai_config["api_key"])

    if use_nvidia:
        print("  ‚úì NVIDIA AI configured")
        print("  üí° Note: Async API not available, using mock analysis")
        use_nvidia = False

    for i, article in enumerate(articles):
        title = article.get("title", "")
        print(f"  üìÑ [{i + 1}/{len(articles)}] {title[:40]}...")

        # Generate analysis
        score = 70 + (i % 30)

        article["ai_score"] = score
        article["ai_summary"] = (
            f"AIÂàÜÊûêÊëòË¶Å: ËøôÊòØ‰∏ÄÁØáÂÖ≥‰∫é{title}ÁöÑÊäÄÊúØÊñáÁ´†„ÄÇÊñáÁ´†Ê∂µÁõñ‰∫∫Â∑•Êô∫ËÉΩ„ÄÅÊú∫Âô®Â≠¶‰π†Á≠âÁÉ≠Èó®ËØùÈ¢òÔºåÂØπÂºÄÂèëËÄÖÊúâÈáçË¶ÅÂèÇËÄÉ‰ª∑ÂÄº„ÄÇ"
        )
        article["ai_category"] = article.get("category", "AI")
        article["translated_title"] = f"[AIÂàÜÊûê] {title}"
        article["key_points"] = [
            "ÊñáÁ´†‰∏ªÈ¢òÊòéÁ°ÆÔºåËÆ®ËÆ∫AIÊäÄÊúØÂèëÂ±ï",
            "ÂåÖÂê´ÂÆûÈôÖÊ°à‰æãÂíå‰ª£Á†ÅÁ§∫‰æã",
            "ÂØπÂºÄÂèëËÄÖÊúâÂÆûÈôÖÊåáÂØºÊÑè‰πâ",
        ]
        article["tags"] = ["AI", "ÊäÄÊúØÊñáÁ´†", article.get("category", "AI")]
        article["difficulty"] = "Intermediate"
        article["reading_time"] = 5 + (i % 5)

        print(f"    ‚úì Score: {score}/100")
        time.sleep(0.1)

    print(f"ü§ñ AI: {len(articles)} articles analyzed\n")
    return articles


def filter_by_quality(articles, min_score=60):
    """Filter articles by quality score"""
    print(f"‚≠ê Filtering (min_score={min_score})...")

    before = len(articles)
    filtered = [a for a in articles if a.get("ai_score", 0) >= min_score]
    filtered.sort(key=lambda x: x.get("ai_score", 0), reverse=True)

    print(f"  ‚úì {len(filtered)}/{before} articles passed filter\n")
    return filtered


def publish_to_notion(articles):
    """Publish to Notion using HTTP"""
    print("üìù Publishing to Notion...")

    # Load Notion config
    notion_key = os.getenv("notion_key")
    database_id = os.getenv("NOTION_DATABASE_ID")

    if (
        not notion_key
        or not database_id
        or database_id == "ËØ∑Âú®ËøôÈáåÂ°´ÂÖ•‰Ω†ÁöÑNotionÊï∞ÊçÆÂ∫ìID"
    ):
        print("  ‚ö†Ô∏è Notion not configured")
        print("  üí° Required env vars:")
        print("     - notion_key (Integration Token)")
        print("     - NOTION_DATABASE_ID")

        # Show what would be published
        print(f"\n  üìã Top articles that would be published:")
        for i, article in enumerate(articles[:5]):
            title = (
                article.get("translated_title") or article.get("title", "Untitled")[:50]
            )
            score = article.get("ai_score", 0)
            print(f"     {i + 1}. {title} (Score: {score})")

        print(f"\n  üí° To enable Notion sync:")
        print("     1. Get your Database ID from Notion URL")
        print("     2. Add to .env file: NOTION_DATABASE_ID=your_id")
        print("     3. Run this script again")

        return 0

    print(f"  ‚úì Notion configured, attempting to publish...")

    # Try to publish via Notion API
    try:
        http_client = SimpleHTTPClient()

        url = "https://api.notion.com/v1/pages"
        headers = {
            "Authorization": f"Bearer {notion_key}",
            "Content-Type": "application/json",
            "Notion-Version": "2022-06-28",
        }

        published = 0
        for i, article in enumerate(articles):
            title = (
                article.get("translated_title")
                or article.get("title", "Untitled")[:100]
            )

            print(f"  üìù [{i + 1}/{len(articles)}] {title[:40]}...")

            data = {
                "parent": {"database_id": database_id},
                "properties": {
                    "Title": {"title": [{"text": {"content": title}}]},
                    "Link": {"url": article.get("link", "")},
                    "Author": {
                        "rich_text": [
                            {"text": {"content": article.get("author", "Unknown")}}
                        ]
                    },
                    "Score": {"number": article.get("ai_score", 0)},
                    "Category": {"select": {"name": article.get("category", "AI")}},
                },
                "children": [
                    {
                        "object": "block",
                        "type": "paragraph",
                        "paragraph": {
                            "rich_text": [
                                {
                                    "text": {
                                        "content": article.get("ai_summary", "")[:1900]
                                    }
                                }
                            ]
                        },
                    }
                ],
            }

            req = urllib.request.Request(
                url,
                data=json.dumps(data).encode("utf-8"),
                headers=headers,
                method="POST",
            )

            try:
                with urllib.request.urlopen(
                    req, timeout=30, context=http_client.ctx
                ) as response:
                    if response.status == 200:
                        published += 1
                        print(f"    ‚úì Published!")
            except urllib.error.HTTPError as e:
                error_body = e.read().decode("utf-8") if e.fp else ""
                print(f"    ‚ö†Ô∏è Notion API error: {e.code} - {error_body[:100]}")
            except Exception as e:
                print(f"    ‚ùå Error: {e}")

            time.sleep(0.5)

        print(f"\n  ‚úì Published {published}/{len(articles)} articles to Notion")
        return published

    except Exception as e:
        print(f"  ‚ùå Notion publishing failed: {e}")
        return 0


def print_summary(collected, analyzed, filtered, published):
    """Print summary"""
    print("\n" + "=" * 60)
    print("üìä Summary")
    print("=" * 60)
    print(f"  Collected: {collected}")
    print(f"  Analyzed:  {analyzed}")
    print(f"  Filtered:  {filtered}")
    print(f"  Published: {published}")
    print(f"\n  ‚è∞ Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)


def main():
    """Main entry point"""
    print_banner()

    # Load config
    config = load_config()
    if not config:
        sys.exit(1)

    # Parse arguments
    limit = 15
    sources = ["reddit", "rss"]

    for arg in sys.argv[1:]:
        if arg.isdigit():
            limit = int(arg)

    # Initialize HTTP client
    http_client = SimpleHTTPClient()

    # Collect articles
    all_articles = []

    if "reddit" in sources:
        reddit_articles = collect_reddit_articles(config, http_client)
        all_articles.extend(reddit_articles)

    if "rss" in sources:
        rss_articles = collect_rss_articles(config, http_client)
        all_articles.extend(rss_articles)

    if not all_articles:
        print("‚ùå No articles collected")
        sys.exit(1)

    # Limit
    if limit and len(all_articles) > limit:
        all_articles = all_articles[:limit]
        print(f"üìå Limited to {limit} articles\n")

    # AI Analysis
    analyzed_articles = analyze_with_ai(all_articles)

    # Quality filter
    min_score = config.get("processing", {}).get("min_score_threshold", 60)
    filtered_articles = filter_by_quality(analyzed_articles, min_score)

    # Publish to Notion
    published_count = publish_to_notion(filtered_articles)

    # Summary
    print_summary(
        collected=len(all_articles),
        analyzed=len(analyzed_articles),
        filtered=len(filtered_articles),
        published=published_count,
    )

    if published_count > 0:
        print("\nüéâ Successfully published to Notion!")
        print("üì± Check your Notion database.")
    else:
        print("\n‚ö†Ô∏è No articles published to Notion.")
        print("üí° Provide NOTION_DATABASE_ID in .env to enable sync.")
        print("\nüìã Articles collected and analyzed (ready to publish):")
        for i, article in enumerate(filtered_articles[:5]):
            print(f"   {i + 1}. {article.get('title', 'Untitled')[:60]}...")

        # Save articles for later publishing
        print("\nüíæ Saving articles for later publish...")
        articles_file = Path(__file__).parent / "collected_articles.json"
        with open(articles_file, "w", encoding="utf-8") as f:
            json.dump(filtered_articles, f, ensure_ascii=False, indent=2)
        print(f"  ‚úì Saved {len(filtered_articles)} articles to {articles_file}")
        print("\nüöÄ To publish, run:")
        print("   python .claude/workflows/notion_manager.py")

    sys.exit(0 if published_count > 0 else 0)  # Always exit 0 for now


if __name__ == "__main__":
    main()
